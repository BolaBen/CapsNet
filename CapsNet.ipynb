{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop((28, 28), padding=2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "trn_dataset = torchvision.datasets.MNIST('.', train=True, download=True, transform=transforms)\n",
    "tst_dataset = torchvision.datasets.MNIST('.', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trn_loader = torch.utils.data.DataLoader(trn_dataset, batch_size, shuffle=True)\n",
    "tst_loader = torch.utils.data.DataLoader(tst_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = self.assemble_decoder(in_features, out_features)\n",
    "    \n",
    "    def assemble_decoder(self, in_features, out_features):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, out_features),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = x[np.arange(0, x.size()[0]), y.cpu().data.numpy(), :].cuda()\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Norm, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.norm(x, p=2, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Routing(torch.nn.Module):\n",
    "    def __init__(self, caps_size_before, caps_size_after, n_capsules_before, n_capsules_after):\n",
    "        super(Routing, self).__init__()\n",
    "        self.n_capsules_before = n_capsules_before\n",
    "        self.n_capsules_after = n_capsules_after\n",
    "        self.caps_size_before = caps_size_before\n",
    "        self.caps_size_after = caps_size_after\n",
    "        \n",
    "        n_in = 1152 * 10 * 16 * 8\n",
    "        variance = 2 / (n_in)\n",
    "        std = np.sqrt(variance)\n",
    "        self.W = torch.nn.Parameter(torch.randn(1152, 10, 16, 8) * std, requires_grad=True)\n",
    "    \n",
    "    # Equation (1)\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = torch.norm(s, p=2, dim=-1, keepdim=True)\n",
    "        s_norm2 = torch.pow(s_norm, 2)\n",
    "        v = (s_norm2 / (1.0 + s_norm2)) * (s / s_norm)\n",
    "        return v\n",
    "    \n",
    "    # Equation (2)\n",
    "    def affine(self, x):\n",
    "        x = self.W @ x.unsqueeze(2).expand(-1, -1, 10, -1).unsqueeze(-1)\n",
    "        return x.squeeze()\n",
    "    \n",
    "    # Equation (3)\n",
    "    @staticmethod\n",
    "    def softmax(x, dim=-1):\n",
    "        exp = torch.exp(x)\n",
    "        return exp / torch.sum(exp, dim, keepdim=True)\n",
    "    \n",
    "    # Procedure 1 - Routing algorithm.\n",
    "    def routing(self, u, r, l):\n",
    "        b = Variable(torch.zeros(u.size()[0], l[0], l[1]), requires_grad=False).cuda() # torch.Size([256, 1152, 10])\n",
    "        \n",
    "        for iteration in range(r):\n",
    "            c = Routing.softmax(b) # torch.Size([256, 1152, 10])\n",
    "            s = (c.unsqueeze(-1).expand(-1, -1, -1, u.size()[-1]) * u).sum(1) # torch.Size([256, 1152, 16])\n",
    "            v = Routing.squash(s) # torch.Size([256, 10, 16])\n",
    "            b += (u * v.unsqueeze(1).expand(-1, l[0], -1, -1)).sum(-1)\n",
    "        return v\n",
    "    \n",
    "    def forward(self, x, n_routing_iter):\n",
    "        x = x.view((-1, self.n_capsules_before, self.caps_size_before))\n",
    "        x = self.affine(x) # torch.Size([256, 1152, 10, 16])\n",
    "        x = self.routing(x, n_routing_iter, (self.n_capsules_before, self.n_capsules_after))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsules(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        self.capsule_dim = 8\n",
    "        self.out_channels = 32\n",
    "        self.input_shape = (256, 20, 20)\n",
    "        self.kernel_size = 9\n",
    "        self.stride = 2\n",
    "        self.in_channels = self.input_shape[0]\n",
    "        \n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            self.in_channels,\n",
    "            self.out_channels * self.capsule_dim,\n",
    "            self.kernel_size,\n",
    "            self.stride\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x = x.view(-1, x.size()[1], x.size()[2], self.out_channels, self.capsule_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(torch.nn.Module):\n",
    "    def __init__(self, input_shape, n_routing_iter):\n",
    "        super(CapsNet, self).__init__()\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.n_routing_iter = n_routing_iter\n",
    "        \n",
    "        self.conv1 = self.assemble_conv1(input_shape[0], 256, 9)\n",
    "        self.primary_capsules = PrimaryCapsules()\n",
    "        self.routing = Routing(8, 16, 6 * 6 * 32, 10)\n",
    "        self.norm = Norm()\n",
    "        self.decoder = Decoder(16, int(np.prod(input_shape)))\n",
    "    \n",
    "    def n_parameters(self):\n",
    "        return np.sum([np.prod(x.size()) for x in self.parameters()])\n",
    "    \n",
    "    def assemble_conv1(self, in_channels, out_channels, kernel_size):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        conv1 = self.conv1(x)\n",
    "        primary_capsules = self.primary_capsules(conv1)\n",
    "        digit_caps = self.routing(primary_capsules, self.n_routing_iter)\n",
    "        scores = self.norm(digit_caps)\n",
    "        reconstruction = None if y is None else self.decoder(digit_caps, y).view((-1,) + self.input_shape)\n",
    "        return scores, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CapsNet(input_shape=(1, 28, 28), n_routing_iter=3).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Parameters: %d' % model.n_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n",
    "    if (y.is_cuda):\n",
    "        return new_y.cuda()\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(torch.nn.Module):\n",
    "    def __init__(self, m_pos=0.9, m_neg=0.1, lamb=0.5):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    # Equation (4)\n",
    "    def forward(self, scores, y):\n",
    "        y = Variable(to_categorical(y, 10))\n",
    "        \n",
    "        Tc = y.float()\n",
    "        loss_pos = torch.pow(torch.clamp(self.m_pos - scores, min=0), 2)\n",
    "        loss_neg = torch.pow(torch.clamp(scores - self.m_neg, min=0), 2)\n",
    "        loss = Tc * loss_pos + self.lamb * (1 - Tc) * loss_neg\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumSquaredDifferencesLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SumSquaredDifferencesLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x_reconstruction, x):\n",
    "        loss = torch.pow(x - x_reconstruction, 2).sum(-1).sum(-1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNetLoss(torch.nn.Module):\n",
    "    def __init__(self, reconstruction_loss_scale=0.0005):\n",
    "        super(CapsNetLoss, self).__init__()\n",
    "        self.digit_existance_criterion = MarginLoss()\n",
    "        self.digit_reconstruction_criterion = SumSquaredDifferencesLoss()\n",
    "        self.reconstruction_loss_scale = reconstruction_loss_scale\n",
    "    \n",
    "    def forward(self, x, y, x_reconstruction, scores):\n",
    "        margin_loss = self.digit_existance_criterion(y_pred.cuda(), y)\n",
    "        reconstruction_loss = self.reconstruction_loss_scale *\\\n",
    "                              self.digit_reconstruction_criterion(x_reconstruction, x)\n",
    "        loss = margin_loss + reconstruction_loss\n",
    "        return loss, margin_loss, reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CapsNetLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(optimizer, learning_rate, global_step, decay_steps, decay_rate, staircase=False):\n",
    "    if (staircase):\n",
    "        decayed_learning_rate = learning_rate * np.power(decay_rate, global_step // decay_steps)\n",
    "    else:\n",
    "        decayed_learning_rate = learning_rate * np.power(decay_rate, global_step / decay_steps)\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_learning_rate\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, train_accuracy, test_accuracy, model, optimizer, path=None):\n",
    "    if (path is None):\n",
    "        path = 'checkpoint-%f-%04d.pth' % (test_accuracy, epoch)\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(model, x, y, x_reconstruction, y_pred):\n",
    "    x = x.squeeze().cpu().data.numpy()\n",
    "    y = y.cpu().data.numpy()[0]\n",
    "    x_reconstruction = x_reconstruction.squeeze().cpu().data.numpy()\n",
    "    _, y_pred = torch.max(y_pred, -1)\n",
    "    y_pred = y_pred.cpu().data.numpy()[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(x, cmap='Greys')\n",
    "    ax[0].set_title('Input: %d' % y)\n",
    "    ax[1].imshow(x_reconstruction, cmap='Greys')\n",
    "    ax[1].set_title('Output: %d' % y_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    metrics = defaultdict(lambda:list())\n",
    "    for batch_id, (x, y) in tqdm_notebook(enumerate(loader), total=len(loader)):\n",
    "        x = Variable(x).float().cuda()\n",
    "        y = Variable(y).cuda()\n",
    "        y_pred, x_reconstruction = model(x, y)\n",
    "        _, y_pred = torch.max(y_pred, -1)\n",
    "        metrics['accuracy'].append((y_pred == y).cpu().data.numpy())\n",
    "    metrics['accuracy'] = np.concatenate(metrics['accuracy']).mean()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tst_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_epoch = 0\n",
    "global_step = 0\n",
    "history = defaultdict(lambda:list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch %d (%d/%d):' % (global_epoch + 1, epoch + 1, n_epochs))\n",
    "    optimizer = exponential_decay(optimizer, 0.001, global_step, 100, 0.99)\n",
    "    \n",
    "    for batch_id, (x, y) in tqdm_notebook(enumerate(trn_loader), total=len(trn_loader)):\n",
    "        x = Variable(x).float().cuda()\n",
    "        y = Variable(y).cuda()\n",
    "        \n",
    "        y_pred, x_reconstruction = model(x, y)\n",
    "        loss, margin_loss, reconstruction_loss = criterion(x, y, x_reconstruction, y_pred.cuda())\n",
    "        \n",
    "        history['margin_loss'].append(margin_loss.cpu().data.numpy()[0])\n",
    "        history['reconstruction_loss'].append(reconstruction_loss.cpu().data.numpy()[0])\n",
    "        history['loss'].append(loss.cpu().data.numpy()[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "    trn_metrics = test(model, trn_loader)\n",
    "    tst_metrics = test(model, tst_loader)\n",
    "    \n",
    "    print('Margin Loss: %f' % history['margin_loss'][-1])\n",
    "    print('Reconstruction Loss: %f' % history['reconstruction_loss'][-1])\n",
    "    print('Loss: %f' % history['loss'][-1])\n",
    "    print('Train Accuracy: %f' % trn_metrics['accuracy'])\n",
    "    print('Test Accuracy: %f' % tst_metrics['accuracy'])\n",
    "    \n",
    "    print('Example:')\n",
    "    idx = np.random.randint(0, len(x))\n",
    "    show_example(model, x[idx], y[idx], x_reconstruction[idx], y_pred[idx])\n",
    "    \n",
    "    if (tst_metrics['accuracy'] >= best_tst_accuracy):\n",
    "        best_tst_accuracy = tst_metrics['accuracy']\n",
    "        save_checkpoint(\n",
    "            global_epoch + 1,\n",
    "            trn_metrics['accuracy'],\n",
    "            tst_metrics['accuracy'],\n",
    "            model,\n",
    "            optimizer\n",
    "        )\n",
    "    global_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "n_points = 1000\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "rolling_mean = np.convolve(np.asarray(history['loss'])[-n_points:],\n",
    "                           np.ones((N,))/N,\n",
    "                           mode='valid')\n",
    "plt.plot(rolling_mean, '-g')\n",
    "\n",
    "rolling_mean = np.convolve(np.asarray(history['margin_loss'])[-n_points:],\n",
    "                           np.ones((N,))/N,\n",
    "                           mode='valid')\n",
    "plt.plot(rolling_mean, '-b')\n",
    "\n",
    "rolling_mean = np.convolve(np.asarray(history['reconstruction_loss'])[-n_points:],\n",
    "                           np.ones((N,))/N,\n",
    "                           mode='valid')\n",
    "plt.plot(rolling_mean, '-r')\n",
    "\n",
    "plt.legend(['Total Loss', 'Margin Loss', 'Reconstruction Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "177px",
    "width": "219px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
